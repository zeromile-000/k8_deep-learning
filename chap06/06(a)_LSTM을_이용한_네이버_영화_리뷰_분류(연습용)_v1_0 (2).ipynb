{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kqq2xWfmYHn2",
        "outputId": "38f794df-51ae-44d6-95c1-5ebef0e8ac43"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting konlpy\n",
            "  Downloading konlpy-0.6.0-py2.py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting JPype1>=0.7.0 (from konlpy)\n",
            "  Downloading jpype1-1.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from konlpy) (5.3.0)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.10/dist-packages (from konlpy) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from JPype1>=0.7.0->konlpy) (24.2)\n",
            "Downloading konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.4/19.4 MB\u001b[0m \u001b[31m31.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jpype1-1.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (493 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m493.8/493.8 kB\u001b[0m \u001b[31m26.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: JPype1, konlpy\n",
            "Successfully installed JPype1-1.5.1 konlpy-0.6.0\n"
          ]
        }
      ],
      "source": [
        "!pip install konlpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "qpN8-KIAYAe_"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "import urllib.request\n",
        "from konlpy.tag import Okt\n",
        "from tqdm import tqdm\n",
        "from collections import Counter\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "53rVuC0sYEsC",
        "outputId": "c9b8c89a-dbe4-42f7-d64a-0cc6119b7a27"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('ratings_test.txt', <http.client.HTTPMessage at 0x79ecc4c65060>)"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt\", filename=\"ratings_train.txt\")\n",
        "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings_test.txt\", filename=\"ratings_test.txt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "MhKLy7cuYPVz"
      },
      "outputs": [],
      "source": [
        "train_data = pd.read_table(\"ratings_train.txt\")\n",
        "test_data = pd.read_table(\"ratings_test.txt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "VrwVs6P0YVhs"
      },
      "outputs": [],
      "source": [
        "train_data.drop_duplicates(subset=[\"document\"], inplace=True)\n",
        "train_data[\"document\"] = train_data[\"document\"].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\", regex=True)\n",
        "train_data[\"document\"] = train_data[\"document\"].str.replace(\"^ +\", \"\", regex=True)\n",
        "train_data[\"document\"] = train_data[\"document\"].replace(\"\", np.nan)\n",
        "train_data = train_data.dropna(how = \"any\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "j1lk42RxY5KX"
      },
      "outputs": [],
      "source": [
        "test_data.drop_duplicates(subset = [\"document\"], inplace=True)\n",
        "test_data[\"document\"] = test_data[\"document\"].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\", regex=True)\n",
        "test_data[\"document\"] = test_data[\"document\"].str.replace(\"^ +\", \"\", regex=True)\n",
        "test_data[\"document\"] = test_data[\"document\"].replace(\"\", np.nan)\n",
        "test_data = test_data.dropna(how=\"any\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "baMKPYAZZ0st",
        "outputId": "7824da20-f5a8-4b85-e890-d662d11d1604"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "전처리 샘플의 개수 : 145393 48852\n"
          ]
        }
      ],
      "source": [
        "print('전처리 샘플의 개수 :',len(train_data),len(test_data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ierYosozaG4e",
        "outputId": "0b4ed4f1-df3e-445c-d1ca-bebb019b9d00"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 145393/145393 [16:33<00:00, 146.34it/s]\n"
          ]
        }
      ],
      "source": [
        "stopwords = ['도', '는', '다', '의', '가', '이', '은', '한', '에', '하', '고', '을', '를', '인', '듯', '과', '와', '네', '들', '듯', '지', '임', '게']\n",
        "okt = Okt()\n",
        "X_train = []\n",
        "for sentence in tqdm(train_data['document']):\n",
        "    tokenized_sentence = okt.morphs(sentence)\n",
        "    stopwords_removed_sentence = [word for word in tokenized_sentence if not word in stopwords]\n",
        "    X_train.append(stopwords_removed_sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "0gBhKCn6b9fL"
      },
      "outputs": [],
      "source": [
        "with open('X_train.pickle', 'wb') as f:\n",
        "    pickle.dump(X_train, f, pickle.HIGHEST_PROTOCOL)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3vA4ay0Da2K1",
        "outputId": "cdba3e47-a573-408e-86da-d6919bda7ebf"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 48852/48852 [05:00<00:00, 162.83it/s]\n"
          ]
        }
      ],
      "source": [
        "X_test = []\n",
        "for sentence in tqdm(test_data['document']):\n",
        "    tokenized_sentence = okt.morphs(sentence) # 토큰화\n",
        "    stopwords_removed_sentence = [word for word in tokenized_sentence if not word in stopwords] # 불용어 제거\n",
        "    X_test.append(stopwords_removed_sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "64sZJrZ_cFQs"
      },
      "outputs": [],
      "source": [
        "with open('X_test.pickle', 'wb') as f:\n",
        "    pickle.dump(X_test, f, pickle.HIGHEST_PROTOCOL)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "AHOoFYjpchmC"
      },
      "outputs": [],
      "source": [
        "y_train = np.array(train_data['label'])\n",
        "y_test = np.array(test_data['label'])\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.2, random_state=0, stratify=y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "s_73rzLrcoIz"
      },
      "outputs": [],
      "source": [
        "word_list = []\n",
        "for sent in X_train:\n",
        "    for word in sent:\n",
        "      word_list.append(word)\n",
        "word_counts = Counter(word_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WyaMGb6nc2Lu",
        "outputId": "d93cf1f7-dac6-467a-c537-e027131a6c9b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "총 단어수 : 88276\n",
            "훈련 데이터에서의 단어 영화의 등장 횟수 : 40264\n",
            "훈련 데이터에서의 단어 공감의 등장 횟수 : 784\n"
          ]
        }
      ],
      "source": [
        "print('총 단어수 :', len(word_counts))\n",
        "print('훈련 데이터에서의 단어 영화의 등장 횟수 :', word_counts['영화'])\n",
        "print('훈련 데이터에서의 단어 공감의 등장 횟수 :', word_counts['공감'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "08Hn2HWac5kY",
        "outputId": "7b7dc9c7-41b8-46f4-b09b-c367f78cf032"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "등장 빈도수 상위 10개 단어\n",
            "['영화', '너무', '정말', '만', '적', '진짜', '으로', '로', '점', '에서']\n"
          ]
        }
      ],
      "source": [
        "vocab = sorted(word_counts, key=word_counts.get, reverse=True)\n",
        "print('등장 빈도수 상위 10개 단어')\n",
        "print(vocab[:10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QCr1e6d7c8nV",
        "outputId": "c6f6e00e-7ee6-4c29-bd2c-1560753d57e8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "단어 집합(vocabulary)의 크기 : 88276\n",
            "등장 빈도가 2번 이하인 희귀 단어의 수: 60078\n",
            "단어 집합에서 희귀 단어의 비율: 68.05700303593277\n",
            "전체 등장 빈도에서 희귀 단어 등장 빈도 비율: 5.644509532507002\n"
          ]
        }
      ],
      "source": [
        "threshold = 3\n",
        "total_cnt = len(word_counts) # 단어의 수\n",
        "rare_cnt = 0 # 등장 빈도수가 threshold보다 작은 단어의 개수를 카운트\n",
        "total_freq = 0 # 훈련 데이터의 전체 단어 빈도수 총 합\n",
        "rare_freq = 0 # 등장 빈도수가 threshold보다 작은 단어의 등장 빈도수의 총 합\n",
        "\n",
        "for key, value in word_counts.items():\n",
        "    total_freq = total_freq + value\n",
        "    if(value < threshold):\n",
        "        rare_cnt = rare_cnt + 1\n",
        "        rare_freq = rare_freq + value\n",
        "\n",
        "print('단어 집합(vocabulary)의 크기 :',total_cnt)\n",
        "print('등장 빈도가 %s번 이하인 희귀 단어의 수: %s'%(threshold - 1, rare_cnt))\n",
        "print(\"단어 집합에서 희귀 단어의 비율:\", (rare_cnt / total_cnt)*100)\n",
        "print(\"전체 등장 빈도에서 희귀 단어 등장 빈도 비율:\", (rare_freq / total_freq)*100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YWe39MXRdA_1",
        "outputId": "b43356d0-8ab1-49b4-c5f8-08a409375997"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "단어 집합의 크기 : 28198\n"
          ]
        }
      ],
      "source": [
        "# 전체 단어 개수 중 빈도수 2이하인 단어는 제거.\n",
        "vocab_size = total_cnt - rare_cnt\n",
        "vocab = vocab[:vocab_size]\n",
        "print('단어 집합의 크기 :', len(vocab))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EmJTnbbOdFE1",
        "outputId": "f6943bd4-dc31-4989-865f-30ea88242fcf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "패딩 토큰과 UNK 토큰을 고려한 단어 집합의 크기 : 28200\n",
            "단어 <PAD>와 맵핑되는 정수 : 0\n",
            "단어 <UNK>와 맵핑되는 정수 : 1\n",
            "단어 영화와 맵핑되는 정수 : 2\n"
          ]
        }
      ],
      "source": [
        "word_to_index = {}\n",
        "word_to_index['<PAD>'] = 0\n",
        "word_to_index['<UNK>'] = 1\n",
        "for index, word in enumerate(vocab) :\n",
        "  word_to_index[word] = index + 2\n",
        "vocab_size = len(word_to_index)\n",
        "print('패딩 토큰과 UNK 토큰을 고려한 단어 집합의 크기 :', vocab_size)\n",
        "print('단어 <PAD>와 맵핑되는 정수 :', word_to_index['<PAD>'])\n",
        "print('단어 <UNK>와 맵핑되는 정수 :', word_to_index['<UNK>'])\n",
        "print('단어 영화와 맵핑되는 정수 :', word_to_index['영화'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "bFp3suUrdSC5"
      },
      "outputs": [],
      "source": [
        "def texts_to_sequences(tokenized_X_data, word_to_index):\n",
        "  encoded_X_data = []\n",
        "  for sent in tokenized_X_data:\n",
        "    index_sequences = []\n",
        "    for word in sent:\n",
        "      try:\n",
        "          index_sequences.append(word_to_index[word])\n",
        "      except KeyError:\n",
        "          index_sequences.append(word_to_index['<UNK>'])\n",
        "    encoded_X_data.append(index_sequences)\n",
        "  return encoded_X_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "XuDFlooIdUvM"
      },
      "outputs": [],
      "source": [
        "encoded_X_train = texts_to_sequences(X_train, word_to_index)\n",
        "encoded_X_valid = texts_to_sequences(X_valid, word_to_index)\n",
        "encoded_X_test = texts_to_sequences(X_test, word_to_index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cupjBDwNdX5_",
        "outputId": "12270497-38dd-4216-edee-2029c255acea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "기존의 첫번째 샘플 : ['이야', '어쩜', '이렇게나', '지루할수가']\n",
            "복원된 첫번째 샘플 : ['이야', '어쩜', '이렇게나', '지루할수가']\n"
          ]
        }
      ],
      "source": [
        "index_to_word = {}\n",
        "for key, value in word_to_index.items():\n",
        "    index_to_word[value] = key\n",
        "decoded_sample = [index_to_word[word] for word in encoded_X_train[0]]\n",
        "print('기존의 첫번째 샘플 :', X_train[0])\n",
        "print('복원된 첫번째 샘플 :', decoded_sample)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0wffyGqUdgjZ",
        "outputId": "b1b66115-0f9e-440f-bb3e-3e52ebfff6a0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "훈련 데이터의 크기 : (116314, 30)\n",
            "검증 데이터의 크기 : (29079, 30)\n",
            "테스트 데이터의 크기 : (48852, 30)\n"
          ]
        }
      ],
      "source": [
        "max_len = 30\n",
        "def pad_sequences(sentences, max_len):\n",
        "  features = np.zeros((len(sentences), max_len), dtype=int)\n",
        "  for index, sentence in enumerate(sentences):\n",
        "    if len(sentence) != 0:\n",
        "      features[index, :len(sentence)] = np.array(sentence)[:max_len]\n",
        "  return features\n",
        "padded_X_train = pad_sequences(encoded_X_train, max_len=max_len)\n",
        "padded_X_valid = pad_sequences(encoded_X_valid, max_len=max_len)\n",
        "padded_X_test = pad_sequences(encoded_X_test, max_len=max_len)\n",
        "print('훈련 데이터의 크기 :', padded_X_train.shape)\n",
        "print('검증 데이터의 크기 :', padded_X_valid.shape)\n",
        "print('테스트 데이터의 크기 :', padded_X_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXcHYXaTdqfF"
      },
      "source": [
        "## LSTM을 사용한 감정 분석"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "RyjGmFUNdtgb"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PZurVmxDwQUm",
        "outputId": "a689ad18-dece-438a-da5b-32e85e77795d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cpu와 cuda 중 다음 기기로 학습함: : cuda\n"
          ]
        }
      ],
      "source": [
        "USE_CUDA = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if USE_CUDA else \"cpu\")\n",
        "print(f\"cpu와 cuda 중 다음 기기로 학습함: : {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AvKQdAu0d2eq",
        "outputId": "bf68bd26-f2d8-4546-fe90-3976e177aceb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([0, 1, 1, 1, 1])\n"
          ]
        }
      ],
      "source": [
        "train_label_tensor = torch.tensor(np.array(y_train))\n",
        "valid_label_tensor = torch.tensor(np.array(y_valid))\n",
        "test_label_tensor = torch.tensor(np.array(y_test))\n",
        "print(train_label_tensor[:5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aNv-SCkTtvPe"
      },
      "source": [
        "아래와 같은 단계를 '레이어 설계'라고 함"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yzts8e8GfscA"
      },
      "outputs": [],
      "source": [
        "# - 문장 길이 = 500\n",
        "# - 배치 크기 = 32\n",
        "# - 데이터 개수 = 2만\n",
        "\n",
        "# - LSTM 매개변수\n",
        "    # - 단어 벡터의 차원 = 100\n",
        "    # - LSTM의 은닉층의 크기 = 128\n",
        "    # - 분류하고자 하는 카테고리 개수 = 2개"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dN3SstcSiuIJ"
      },
      "source": [
        "### 텍스트 분류를 위한 레이어"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "uw7NjHgufybC"
      },
      "outputs": [],
      "source": [
        "# - 단어 벡터의 차원 = 100\n",
        "# - LSTM의 은닉층의 크기 = 128\n",
        "# - 분류하고자 하는 카테고리 개수 = 2개\n",
        "\n",
        "class TextClassifier(nn.Module):\n",
        "  def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):\n",
        "    super().__init__()\n",
        "    self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "    self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
        "    self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "  def forward(self, x):\n",
        "    embedded = self.embedding(x)\n",
        "    lstm_out, (hidden, cell) = self.lstm(embedded)\n",
        "    last_hidden = hidden.squeeze(0)\n",
        "    logits = self.fc(last_hidden)\n",
        "    return logits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46WCmtAoizVD"
      },
      "source": [
        "### 데이터를 텐서로 변경"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "BVkiq9XZguPu"
      },
      "outputs": [],
      "source": [
        "encoded_train = torch.tensor(padded_X_train).to(torch.int64)\n",
        "train_dataset = torch.utils.data.TensorDataset(encoded_train, train_label_tensor)\n",
        "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "encoded_valid = torch.tensor(padded_X_valid).to(torch.int64)\n",
        "valid_dataset = torch.utils.data.TensorDataset(encoded_valid, valid_label_tensor)\n",
        "valid_dataloader = torch.utils.data.DataLoader(valid_dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "encoded_test = torch.tensor(padded_X_test).to(torch.int64)\n",
        "test_dataset = torch.utils.data.TensorDataset(encoded_test, test_label_tensor)\n",
        "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Csl4BRRfCHjE",
        "outputId": "f3866196-e96a-4297-e583-26f9394ed1c1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3635\n"
          ]
        }
      ],
      "source": [
        "totla_batch = len(train_dataloader)\n",
        "print(totla_batch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eYqwC83di4GX"
      },
      "source": [
        "### 배치데이터 구축"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IBqmnAizgy_u",
        "outputId": "ce8e10e8-45b5-4c46-b508-987f0d0b048d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "총 배치의 수 : 3635\n"
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DhFV7w5_i7c1"
      },
      "source": [
        "### 모델 생성"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ymu-WagSg1WT",
        "outputId": "779f5b00-9951-4aa7-b84d-db471b915fbd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "TextClassifier(\n",
              "  (embedding): Embedding(28200, 100)\n",
              "  (lstm): LSTM(100, 128, batch_first=True)\n",
              "  (fc): Linear(in_features=128, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 76,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "embedding_dim = 100\n",
        "hidden_dim = 128\n",
        "output_dim = 2\n",
        "learning_rate = 0.01\n",
        "num_epochs = 10\n",
        "\n",
        "model = TextClassifier(vocab_size, embedding_dim, hidden_dim, output_dim)\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tPT49bx7i-Qh"
      },
      "source": [
        "### optimizer 등 설정"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "kdNpkJ_Ug4S9"
      },
      "outputs": [],
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rHzAOq7NjBge"
      },
      "source": [
        "### 정확도 계산"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "EfJ19IVGg7L4"
      },
      "outputs": [],
      "source": [
        "def calculate_accuracy(logits, labels):\n",
        "  predicted = torch.argmax(logits, dim=1)\n",
        "  correct = (predicted == labels).sum().item()\n",
        "  total = labels.size(0)\n",
        "  accuracy = correct / total\n",
        "  return accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AMxfD0HQjDPk"
      },
      "source": [
        "### 학습 및 계산"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "08rQUG31g-dx"
      },
      "outputs": [],
      "source": [
        "# 모델, valid_date, criterion(loss_fn), device\n",
        "def evaluate(model, valid_dataloader, criterion, device):\n",
        "  val_loss = 0\n",
        "  val_correct = 0\n",
        "  val_total = 0\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    for batch_X, batch_y in valid_dataloader:\n",
        "      batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
        "      logits = model(batch_X)\n",
        "      loss = criterion(logits, batch_y)\n",
        "      val_loss += loss.item()\n",
        "      val_correct += calculate_accuracy(logits, batch_y)*batch_y.size(0)\n",
        "      val_total += batch_y.size(0)\n",
        "    val_accuracy = val_correct / val_total\n",
        "    val_loss = val_loss / len(valid_dataloader)\n",
        "  return val_loss, val_accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0-ZFL1aSjQ3G"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qt20qA7ChFKx",
        "outputId": "78d81964-3937-4ca2-b9fe-bbaab78e2768"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch : 1/10\n",
            "Train Loss : 0.5064249525945813 / Train Accuracy : 0.7213\n",
            "Valid Loss : 0.38620742070268355 / Valid Accuracy : 0.8231713607758177\n",
            "Epoch : 2/10\n",
            "Train Loss : 0.3301177739031213 / Train Accuracy : 0.8544\n",
            "Valid Loss : 0.3538238086601426 / Valid Accuracy : 0.8407097905705148\n",
            "Epoch : 3/10\n",
            "Train Loss : 0.2606281588153793 / Train Accuracy : 0.8905\n",
            "Valid Loss : 0.3577466825283531 / Valid Accuracy : 0.8442862546855119\n",
            "Epoch : 4/10\n",
            "Train Loss : 0.19829159711207042 / Train Accuracy : 0.9201\n",
            "Valid Loss : 0.39572288965595304 / Valid Accuracy : 0.843117026032532\n",
            "Epoch : 5/10\n",
            "Train Loss : 0.14312548631625416 / Train Accuracy : 0.9446\n",
            "Valid Loss : 0.474133671571811 / Valid Accuracy : 0.8386120568107569\n",
            "Epoch : 6/10\n",
            "Train Loss : 0.09916067979153774 / Train Accuracy : 0.9627\n",
            "Valid Loss : 0.5561835277944442 / Valid Accuracy : 0.8346917019154716\n",
            "Epoch : 7/10\n",
            "Train Loss : 0.07018199951368406 / Train Accuracy : 0.9743\n",
            "Valid Loss : 0.6433495403532804 / Valid Accuracy : 0.832731524467829\n",
            "Epoch : 8/10\n",
            "Train Loss : 0.05335149680141954 / Train Accuracy : 0.9804\n",
            "Valid Loss : 0.7181002442183012 / Valid Accuracy : 0.8305650125520134\n",
            "Epoch : 9/10\n",
            "Train Loss : 0.04298594721823231 / Train Accuracy : 0.9846\n",
            "Valid Loss : 0.7765797566442174 / Valid Accuracy : 0.8297740637573506\n",
            "Epoch : 10/10\n",
            "Train Loss : 0.03667150187930858 / Train Accuracy : 0.9869\n",
            "Valid Loss : 0.8917215317508208 / Valid Accuracy : 0.8297396746793219\n"
          ]
        }
      ],
      "source": [
        "best_val_loss = float('inf')\n",
        "for epoch in range(10):\n",
        "  train_loss = 0\n",
        "  train_correct = 0\n",
        "  train_total = 0\n",
        "  model.train()\n",
        "  for batch_X, batch_y in train_dataloader:\n",
        "    batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
        "    logits = model(batch_X)\n",
        "    loss = criterion(logits, batch_y)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    train_loss += loss.item()\n",
        "    train_correct += calculate_accuracy(logits, batch_y)*batch_y.size(0)\n",
        "    train_total += batch_y.size(0)\n",
        "  train_accuracy = train_correct / train_total\n",
        "  train_loss = train_loss / len(train_dataloader)\n",
        "\n",
        "  val_loss, val_accuracy = evaluate(model, valid_dataloader, criterion, device)\n",
        "  print(f\"Epoch : {epoch+1}/{num_epochs}\")\n",
        "  print(f\"Train Loss : {train_loss} / Train Accuracy : {train_accuracy:.4f}\")\n",
        "  print(f\"Valid Loss : {val_loss} / Valid Accuracy : {val_accuracy}\")\n",
        "  if val_loss < best_val_loss:\n",
        "    best_val_loss = val_loss\n",
        "    torch.save(model.state_dict(), 'best_model_checkpoint.pt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ELf6pHHxjR5F"
      },
      "source": [
        "### 모델 불러오기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HKGuxDSIhOTo",
        "outputId": "b717bdac-ea95-4169-f415-483b0cce1d92"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-81-7501090c7362>:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load('best_model_checkpoint.pt'))\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "TextClassifier(\n",
              "  (embedding): Embedding(28200, 100)\n",
              "  (lstm): LSTM(100, 128, batch_first=True)\n",
              "  (fc): Linear(in_features=128, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 81,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.load_state_dict(torch.load('best_model_checkpoint.pt'))\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71DPSUWgjVyg"
      },
      "source": [
        "### 측정"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "4KLAMDerhVFv"
      },
      "outputs": [],
      "source": [
        "index_to_tag ={0 : \"부정\", 1 : \"긍정\"}\n",
        "\n",
        "def predict(text, model, word_to_index, index_to_tag):\n",
        "  model.eval()\n",
        "  tokens = okt.morphs(text)\n",
        "  tokens = [word for word in tokens if not word in stopwords]\n",
        "  token_indices = [word_to_index.get(token, 1)  for token in tokens]\n",
        "  input_tensor = torch.tensor([token_indices], dtype=torch.int64).to(device)\n",
        "  with torch.no_grad():\n",
        "    logits = model(input_tensor)\n",
        "  predicted_index = torch.argmax(logits, dim=1)\n",
        "  predicted_tag = index_to_tag[predicted_index.item()]\n",
        "  return predicted_tag\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "z3t49KwuPMCy",
        "outputId": "4987735d-d0e5-4153-d0fc-2fec4d76c257"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'부정'"
            ]
          },
          "execution_count": 83,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "predict(\"이 영화 별로에요ㅠㅠ\", model, word_to_index, index_to_tag)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "XWfqCLA0PEVK",
        "outputId": "cf6b46a5-5006-49d2-fe3d-01dab03d9277"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'긍정'"
            ]
          },
          "execution_count": 84,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "predict(\"이 영화 개꿀!\", model, word_to_index, index_to_tag)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "torch_book",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
